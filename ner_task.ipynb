{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the multinerd the English documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# loading the dataset\n",
    "dataset = load_dataset(\"Babelscape/multinerd\")\n",
    "dataset = dataset.filter(lambda example: example['lang'] == 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Giving more memory spaces while the training by changing some env variables for Mac M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_MPS_HIGH_WATERMARK_RATIO: 0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set the environment variable\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Now, you can access the environment variable if needed\n",
    "value = os.environ.get(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\")\n",
    "print(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO:\", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets \n",
    "import numpy as np \n",
    "from transformers import BertTokenizerFast \n",
    "from transformers import DataCollatorForTokenClassification \n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer \n",
    "\n",
    "# using the seqeval for our evaluating the models\n",
    "metric = datasets.load_metric(\"seqeval\")\n",
    "\n",
    "def tokenize_and_align_labels(examples,tokenizer, label_all_tokens=True): \n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) \n",
    "    labels = [] \n",
    "    for i, label in enumerate(examples[\"ner_tags\"]): \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) \n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token. \n",
    "        previous_word_idx = None \n",
    "        label_ids = []\n",
    "        # Special tokens like `` and `<\\s>` are originally mapped to None \n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids: \n",
    "            if word_idx is None: \n",
    "                # set –100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token                 \n",
    "                label_ids.append(label[word_idx]) \n",
    "            else: \n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100) \n",
    "                # mask the subword representations after the first subword\n",
    "                 \n",
    "            previous_word_idx = word_idx \n",
    "        labels.append(label_ids) \n",
    "    tokenized_inputs[\"labels\"] = labels \n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, label_list,only_overall):\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics for Named Entity Recognition (NER) predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - eval_preds (tuple): Tuple containing predicted logits and true labels.\n",
    "    - label_list (list): List of NER label names.\n",
    "    - only_overall (bool): If True, returns only overall evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing evaluation metrics including precision, recall, F1 score, and accuracy.\n",
    "\n",
    "    Note:\n",
    "    - The function expects logits and labels to be in the same order.\n",
    "    - It removes values where the label is -100 (padding tokens).\n",
    "    - The label_list should correspond to the NER labels used in the model.\n",
    "    - Evaluation metrics are computed using the `metric` object.\n",
    "    - If only_overall is True, returns only overall evaluation metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    pred_logits, labels = eval_preds \n",
    "    \n",
    "    pred_logits = np.argmax(pred_logits, axis=2) \n",
    "    # the logits and the probabilities are in the same order,\n",
    "    # so we don’t need to apply the softmax\n",
    "    \n",
    "    # We remove all the values where the label is -100\n",
    "    predictions = [ \n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "        for prediction, label in zip(pred_logits, labels) \n",
    "    ] \n",
    "    true_labels = [ \n",
    "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "       for prediction, label in zip(pred_logits, labels) \n",
    "    ] \n",
    "    results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "    if only_overall == True:\n",
    "        return { \n",
    "                \"precision\": results[\"overall_precision\"], \n",
    "                \"recall\": results[\"overall_recall\"], \n",
    "                \"f1\": results[\"overall_f1\"], \n",
    "                \"accuracy\": results[\"overall_accuracy\"], \n",
    "        }\n",
    "    else: # returning the metrics include the overall and the individual performance for each NER token\n",
    "        return results\n",
    "        \n",
    "def fine_tuning_model(dataset, tokenizer_type= \"distilbert-base-cased\",label_list=[], model_type=\"distilbert-base-cased\",system_type='A', evaluate = False):\n",
    "    \"\"\" \n",
    "    Fine-tunes a Token Classification model on the provided dataset using the specified tokenizer and model architecture.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (Dataset): A dataset containing train, validation, and test splits.\n",
    "    - tokenizer_type (str): The type of tokenizer to be used. Defaults to \"distilbert-base-cased\".\n",
    "    - label_list (list): List of NER label names.\n",
    "    - model_type (str): The type of pre-trained model architecture to be used. Defaults to \"distilbert-base-cased\".\n",
    "    - system_type (str): The system identifier (A or B) for saving checkpoints and models.\n",
    "    - evaluate (bool): If True, loads a pre-trained model and evaluates it on the test dataset.\n",
    "\n",
    "    Returns:\n",
    "    - If evaluate is False:\n",
    "        Tuple: (model, tokenizer, trainer, tokenized_datasets)\n",
    "    - If evaluate is True:\n",
    "        TrainerOutput: Results from model evaluation on the test dataset.\n",
    "\n",
    "    Note:\n",
    "    - The function uses the Hugging Face Transformers library.\n",
    "    - If evaluate is False, it performs training, saves the trained model and tokenizer, and returns necessary objects.\n",
    "    - If evaluate is True, it loads a pre-trained model, evaluates it on the test dataset, and returns evaluation results.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(tokenizer_type) \n",
    "\n",
    "    tokenized_datasets = dataset.map(lambda example: tokenize_and_align_labels(example, tokenizer), batched=True)\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    args = TrainingArguments( \n",
    "        f\"models/{system_type}/{model_type}_checkpoints\",\n",
    "        evaluation_strategy = \"epoch\", \n",
    "        learning_rate=2e-5, \n",
    "        per_device_train_batch_size=10, \n",
    "        per_device_eval_batch_size=10, \n",
    "        num_train_epochs=2, \n",
    "        weight_decay=0.01, \n",
    "        )     \n",
    "\n",
    "    if evaluate == False: \n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_type, num_labels=len(label_list))\n",
    "        trainer = Trainer( \n",
    "            model, \n",
    "            args, \n",
    "            train_dataset=tokenized_datasets['train'], \n",
    "            eval_dataset=tokenized_datasets['validation'], \n",
    "            data_collator=data_collator, \n",
    "            tokenizer=tokenizer, \n",
    "            compute_metrics= lambda eval_preds: compute_metrics(eval_preds, label_list, True),\n",
    "            ) \n",
    "        print('Training the model')\n",
    "        trainer.train()\n",
    "        print('Saving the model')\n",
    "        model.save_pretrained('models/' +system_type+'/'+ model_type + \"_ner_model\")\n",
    "        print('Saving the tokenizer')\n",
    "        tokenizer.save_pretrained('models/'+system_type+'/'+ model_type + \"_ner_tokenizer\")\n",
    "        return model, tokenizer, trainer, tokenized_datasets\n",
    "    else: \n",
    "        # Load the saved model\n",
    "        loaded_model_a = AutoModelForTokenClassification.from_pretrained('models/' + system_type + '/' + model_type + \"_ner_model\")\n",
    "        trainer_2 = Trainer( \n",
    "                loaded_model_a, \n",
    "                args, \n",
    "                data_collator=data_collator, \n",
    "                tokenizer=tokenizer, \n",
    "                compute_metrics= lambda eval_preds: compute_metrics(eval_preds, label_list,True),\n",
    "        )\n",
    "        print('making prediction on test dataset')\n",
    "        results = trainer_2.predict(tokenized_datasets['test'])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines label-to-id and id-to-label mappings for Named Entity Recognition (NER) systems A and B.\n",
    "\n",
    "- system_a_label2id: Mapping of NER labels to unique IDs for System A.\n",
    "- system_a_id2label: Mapping of unique IDs to NER labels for System A.\n",
    "\n",
    "- system_b_label2id: Mapping of selected NER labels to unique IDs for System B.\n",
    "                     System B focuses on specific entity types.\n",
    "- system_b_id2label: Mapping of unique IDs to NER labels for System B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_a_label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-ANIM\": 7,\n",
    "    \"I-ANIM\": 8,\n",
    "    \"B-BIO\": 9,\n",
    "    \"I-BIO\": 10,\n",
    "    \"B-CEL\": 11,\n",
    "    \"I-CEL\": 12,\n",
    "    \"B-DIS\": 13,\n",
    "    \"I-DIS\": 14,\n",
    "    \"B-EVE\": 15,\n",
    "    \"I-EVE\": 16,\n",
    "    \"B-FOOD\": 17,\n",
    "    \"I-FOOD\": 18,\n",
    "    \"B-INST\": 19,\n",
    "    \"I-INST\": 20,\n",
    "    \"B-MEDIA\": 21,\n",
    "    \"I-MEDIA\": 22,\n",
    "    \"B-MYTH\": 23,\n",
    "    \"I-MYTH\": 24,\n",
    "    \"B-PLANT\": 25,\n",
    "    \"I-PLANT\": 26,\n",
    "    \"B-TIME\": 27,\n",
    "    \"I-TIME\": 28,\n",
    "    \"B-VEHI\": 29,\n",
    "    \"I-VEHI\": 30,\n",
    "  }\n",
    "\n",
    "system_a_id2label = {id: label for label, id in system_a_label2id.items()}\n",
    "system_b_label2id = {label: id for label, id in system_a_label2id.items() if id <=8 or id in [13,14]}\n",
    "system_b_label2id['B-DIS']=9\n",
    "system_b_label2id['I-DIS']=10\n",
    "system_b_id2label = {id: label for label, id in system_b_label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 623/3291 [32:16<2:18:12,  3.11s/it]65.04 examples/s]\n",
      "Map: 100%|██████████| 262560/262560 [00:07<00:00, 33955.21 examples/s]\n",
      "Map: 100%|██████████| 32820/32820 [00:00<00:00, 36550.78 examples/s]\n",
      "Map: 100%|██████████| 32908/32908 [00:00<00:00, 35031.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_tags(tags):\n",
    "      \"\"\"\n",
    "    Converts Named Entity Recognition (NER) tags in the specified format to a simplified version for System B.\n",
    "\n",
    "    Parameters:\n",
    "    - tags (list): List of NER tags represented as integers.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of converted NER tags based on System B requirements.\n",
    "    \"\"\" \n",
    "    for i in range(len(tags)):\n",
    "        if tags[i]== 13: # corresponds to B-DIS\n",
    "            tags[i] = 9\n",
    "        elif tags[i] == 14: # corresponds to I-DIS\n",
    "            tags[i] = 10\n",
    "        elif tags[i]> 8:    # corresponts to all NER tags except (PERSON(PER), ORGANIZATION(ORG), LOCATION(LOC), DISEASES(DIS), ANIMAL(ANIM))\n",
    "            tags[i] = 0\n",
    "\n",
    "    return tags\n",
    "\n",
    "dataset_a = dataset # for the system A we will take the entier dataset \n",
    "\n",
    "# Apply the convert_tags function to the 'ner_tags' column in your dataset\n",
    "dataset_b = dataset_a.map(lambda example: {'ner_tags': convert_tags(example['ner_tags'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tunes a Token Classification model on the provided dataset using the specified tokenizer and model architecture for System A.\n",
    "\n",
    "Parameters:\n",
    "- dataset_a (Dataset): A dataset containing train, validation, and test splits for System A.\n",
    "- tokenizer_type (str): The type of tokenizer to be used. Defaults to \"distilbert-base-cased\".\n",
    "- label_list (list): List of NER label names for System A.\n",
    "- model_type (str): The type of pre-trained model architecture to be used. Defaults to \"distilbert-base-cased\".\n",
    "- system_type (str): The system identifier (A or B) for saving checkpoints and models.\n",
    "\n",
    "Returns:\n",
    "- Tuple: (model_a, tokenizer_a, trainer_a, tokenized_datasets_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING & EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a, tokenizer_a, trainer_a, tokenized_datasets_a = fine_tuning_model(dataset_a, tokenizer_type= \"distilbert-base-cased\",label_list=system_a_id2label, model_type=\"distilbert-base-cased\",system_type= 'A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tunes a Token Classification model on the provided dataset using the specified tokenizer and model architecture for System B.\n",
    "\n",
    "Parameters:\n",
    "- dataset_b (Dataset): A dataset containing train, validation, and test splits for System B.\n",
    "- tokenizer_type (str): The type of tokenizer to be used. Defaults to \"distilbert-base-cased\".\n",
    "- label_list (list): List of NER label names for System B.\n",
    "- model_type (str): The type of pre-trained model architecture to be used. Defaults to \"distilbert-base-cased\".\n",
    "- system_type (str): The system identifier (A or B) for saving checkpoints and models.\n",
    "\n",
    "Returns:\n",
    "- Tuple: (model_b, tokenizer_b, trainer_b, tokenized_datasets_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b, tokenizer_b, trainer_b, tokenized_datasets_b = fine_tuning_model(dataset_b, tokenizer_type= \"distilbert-base-cased\",label_list=system_b_id2label, model_type=\"distilbert-base-cased\",system_type= 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating System A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates a pre-trained Token Classification model on the test dataset for System A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction on test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3291/3291 [02:41<00:00, 20.34it/s]\n"
     ]
    }
   ],
   "source": [
    "system_a_evaluation_res = fine_tuning_model(dataset_a, tokenizer_type= \"distilbert-base-cased\",label_list=system_a_id2label, model_type=\"distilbert-base-cased\",system_type= 'A', evaluate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming system_a_res is the result obtained from the fine-tuning_model function for System A.\n",
    "\n",
    "# Extract predictions and true labels from the evaluation results\n",
    "a_eval_preds = system_a_res.predictions, system_a_res.label_ids\n",
    "\n",
    "# Define the label list for System A\n",
    "label_list = list(system_a_label2id.keys())\n",
    "\n",
    "# Compute detailed evaluation metrics for System A\n",
    "system_a_eval_metrics = compute_metrics(a_eval_preds, system_a_id2label, only_overall=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANIM</th>\n",
       "      <td>0.725483</td>\n",
       "      <td>0.740347</td>\n",
       "      <td>0.732839</td>\n",
       "      <td>5076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIO</th>\n",
       "      <td>0.549296</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CEL</th>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>0.736238</td>\n",
       "      <td>0.768272</td>\n",
       "      <td>0.751914</td>\n",
       "      <td>8182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVE</th>\n",
       "      <td>0.949861</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.948540</td>\n",
       "      <td>720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOOD</th>\n",
       "      <td>0.648944</td>\n",
       "      <td>0.624686</td>\n",
       "      <td>0.636584</td>\n",
       "      <td>7972.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INST</th>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.741259</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.985702</td>\n",
       "      <td>0.991710</td>\n",
       "      <td>0.988697</td>\n",
       "      <td>24608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIA</th>\n",
       "      <td>0.939123</td>\n",
       "      <td>0.941416</td>\n",
       "      <td>0.940268</td>\n",
       "      <td>2458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MYTH</th>\n",
       "      <td>0.838095</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.952513</td>\n",
       "      <td>0.965421</td>\n",
       "      <td>0.958923</td>\n",
       "      <td>8560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.991287</td>\n",
       "      <td>0.992142</td>\n",
       "      <td>0.991714</td>\n",
       "      <td>20870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLANT</th>\n",
       "      <td>0.645237</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>0.685868</td>\n",
       "      <td>5238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIME</th>\n",
       "      <td>0.770419</td>\n",
       "      <td>0.794989</td>\n",
       "      <td>0.782511</td>\n",
       "      <td>878.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VEHI</th>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.802676</td>\n",
       "      <td>308.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       precision    recall        f1   number\n",
       "ANIM    0.725483  0.740347  0.732839   5076.0\n",
       "BIO     0.549296  0.795918  0.650000     98.0\n",
       "CEL     0.878049  0.857143  0.867470    252.0\n",
       "DIS     0.736238  0.768272  0.751914   8182.0\n",
       "EVE     0.949861  0.947222  0.948540    720.0\n",
       "FOOD    0.648944  0.624686  0.636584   7972.0\n",
       "INST    0.706667  0.779412  0.741259    136.0\n",
       "LOC     0.985702  0.991710  0.988697  24608.0\n",
       "MEDIA   0.939123  0.941416  0.940268   2458.0\n",
       "MYTH    0.838095  0.765217  0.800000    230.0\n",
       "ORG     0.952513  0.965421  0.958923   8560.0\n",
       "PER     0.991287  0.992142  0.991714  20870.0\n",
       "PLANT   0.645237  0.731959  0.685868   5238.0\n",
       "TIME    0.770419  0.794989  0.782511    878.0\n",
       "VEHI    0.827586  0.779221  0.802676    308.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision, Recall, F1 for every ner tags \n",
    "pd.DataFrame(system_a_eval_metrics).T.iloc[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall_recall for system A:     0.8966653424625523\n",
      "Overall_precision for system A:  0.8843078057661727\n",
      "Overall_f1 for system A:         0.8904437018472106\n"
     ]
    }
   ],
   "source": [
    "# overall precision, recall, f1 for all ner tags\n",
    "print('Overall_recall for system A:    ', system_a_eval_metrics['overall_recall'])\n",
    "print('Overall_precision for system A: ', system_a_eval_metrics['overall_precision'])\n",
    "print('Overall_f1 for system A:        ', system_a_eval_metrics['overall_f1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the system B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates a pre-trained Token Classification model on the test dataset for System B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "Map:   0%|          | 0/32820 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 32820/32820 [00:02<00:00, 16134.49 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction on test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3291/3291 [01:45<00:00, 31.30it/s]\n"
     ]
    }
   ],
   "source": [
    "system_b_evaluation_res = fine_tuning_model(dataset_b, tokenizer_type= \"distilbert-base-cased\",label_list=system_b_id2label, model_type=\"distilbert-base-cased\",system_type= 'B', evaluate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming system_b_evaluation_res is the result obtained from the fine_tuning_model function for System B.\n",
    "\n",
    "# Extract predictions and true labels from the evaluation results for System B\n",
    "b_eval_preds = system_b_evaluation_res.predictions, system_b_evaluation_res.label_ids\n",
    "\n",
    "# Define the label list for System B\n",
    "label_list = list(system_b_label2id.keys())\n",
    "\n",
    "\"\"\" \n",
    "Computes detailed evaluation metrics for Named Entity Recognition (NER) predictions on System B.\n",
    "\n",
    "Parameters:\n",
    "- b_eval_preds (tuple): Tuple containing predicted logits and true labels for System B.\n",
    "- label_list (list): List of NER label names for System B.\n",
    "- system_b_id2label (dict): Mapping of unique IDs to NER labels for System B.\n",
    "- only_overall (bool): If False, returns detailed evaluation metrics for each NER label.\n",
    "\n",
    "Returns:\n",
    "- dict: Dictionary containing evaluation metrics for System B, including precision, recall, F1 score, and number of occurrences.\n",
    "\"\"\"\n",
    "# Compute detailed evaluation metrics for System B\n",
    "system_b_eval_metrics = compute_metrics(b_eval_preds, system_b_id2label, only_overall=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANIM</th>\n",
       "      <td>0.731594</td>\n",
       "      <td>0.753208</td>\n",
       "      <td>0.742244</td>\n",
       "      <td>6702.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.809335</td>\n",
       "      <td>0.787630</td>\n",
       "      <td>4028.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.992661</td>\n",
       "      <td>0.992509</td>\n",
       "      <td>0.992585</td>\n",
       "      <td>38978.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.971551</td>\n",
       "      <td>0.971740</td>\n",
       "      <td>0.971646</td>\n",
       "      <td>10262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.982165</td>\n",
       "      <td>0.988442</td>\n",
       "      <td>0.985293</td>\n",
       "      <td>14708.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      precision    recall        f1   number\n",
       "ANIM   0.731594  0.753208  0.742244   6702.0\n",
       "DIS    0.767059  0.809335  0.787630   4028.0\n",
       "LOC    0.992661  0.992509  0.992585  38978.0\n",
       "ORG    0.971551  0.971740  0.971646  10262.0\n",
       "PER    0.982165  0.988442  0.985293  14708.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(system_b_eval_metrics).T.iloc[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall_recall for system B:     0.9574975226974477\n",
      "Overall_precision for system B:  0.9510028195988721\n",
      "Overall_f1 for system B:         0.9542391202807842\n"
     ]
    }
   ],
   "source": [
    "# overall precision, recall, f1 for all ner tags\n",
    "print('Overall_recall for system B:    ', system_b_eval_metrics['overall_recall'])\n",
    "print('Overall_precision for system B: ', system_b_eval_metrics['overall_precision'])\n",
    "print('Overall_f1 for system B:        ', system_b_eval_metrics['overall_f1'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
